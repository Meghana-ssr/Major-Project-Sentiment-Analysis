{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Major Project - Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzwi+MBAkyA3XIdd0QAhfe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meghana-ssr/Major-Project-Sentiment-Analysis/blob/main/Major_Project_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adSEHTqZUSfJ"
      },
      "source": [
        "# Heroku Deployed App :- https://sentiment-analyzis.herokuapp.com/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "zkSe2qDYNMud",
        "outputId": "3d406969-e15b-46fa-8767-df303219f8ca"
      },
      "source": [
        "#Data Set from kaggle :- https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "#Drive link of the data set :- https://drive.google.com/file/d/1nodRisBkwPjEFIEcDnMyA0WFyIiKU_o9/view?usp=sharing\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/IMDB Dataset.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkArTvhsRpD0"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-0kut4WLFnj"
      },
      "source": [
        "#removing punctuations\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skdAn0FnWjU9"
      },
      "source": [
        "#removing HTML Tags\n",
        "from bs4 import BeautifulSoup\n",
        "def html_tag(text):\n",
        "  soup = BeautifulSoup(text,\"html.parser\")\n",
        "  new_text = soup.get_text()\n",
        "  return new_text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emT7rYqYMW8G",
        "outputId": "9d4d4032-f579-4e3b-9fa7-8d9d6f94ab50"
      },
      "source": [
        "#Stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "stop_words.remove('no')\n",
        "stop_words.remove('not')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "def remove_stopwords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    filtered_text = ' '.join(filtered_text)\n",
        "    return filtered_text\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3Gtp9SFMgXx",
        "outputId": "59a286c1-65b4-48d5-b151-b0cf7fe99d27"
      },
      "source": [
        "#Stemming\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    stems = [stemmer.stem(word) for word in word_tokens]\n",
        "    stems = ' '.join(stems)\n",
        "    return stems\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBMUl0-VRka8",
        "outputId": "4a36a9d7-045c-4452-9113-d8db4da393d9"
      },
      "source": [
        "#Lemmetizing\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_word(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "  lemmas = ' '.join(lemmas)\n",
        "  return lemmas  \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ftOo-69Sy0e"
      },
      "source": [
        "#Removing Numbers\n",
        "def remove_numbers(text):\n",
        "    result = re.sub(r'\\d+', '', text)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqh44cLPa9N5"
      },
      "source": [
        "#Lower Case\n",
        "def lower(text):\n",
        "    return text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "V0TYRHV4IGjC",
        "outputId": "11d40d1f-657f-4187-a161-848695f7da52"
      },
      "source": [
        "df.review = df.review.apply(lambda x: x.lower())\n",
        "df.review = df.review.apply(remove_punctuation)\n",
        "df.review = df.review.apply(remove_numbers)\n",
        "df.review = df.review.apply(html_tag)\n",
        "df.review = df.review.apply(stem_words)\n",
        "df.review = df.review.apply(lemmatize_word)\n",
        "df.review = df.review.apply(remove_stopwords)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one review ha mention watch oz episod youll ho...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wonder littl product br br film techniqu veri ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>think thi wa wonder way spend time hot summer ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basic famili littl boy jake think zombi hi clo...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei love time money visual stun film...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  one review ha mention watch oz episod youll ho...  positive\n",
              "1  wonder littl product br br film techniqu veri ...  positive\n",
              "2  think thi wa wonder way spend time hot summer ...  positive\n",
              "3  basic famili littl boy jake think zombi hi clo...  negative\n",
              "4  petter mattei love time money visual stun film...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tCYJNtFT9jn"
      },
      "source": [
        "x = df['review'].values\n",
        "y = df['sentiment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubKa66qb-ycr"
      },
      "source": [
        "#Split Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C07LlwesUez_"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDYbaBaq_a8m"
      },
      "source": [
        "#pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "text_model = Pipeline([('TFIDF',TfidfVectorizer()),('model',MultinomialNB())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSfXB3DYh6Qq",
        "outputId": "de74fb10-acc4-4e20-a787-eadd7ee144ec"
      },
      "source": [
        "#Fitting a Model\n",
        "text_model.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('TFIDF',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('model',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4w7mpU3miP3",
        "outputId": "e15ab870-c23f-4ce7-a81c-42b7694fd5b3"
      },
      "source": [
        "#Checking For New Input\n",
        "text =\"Resurgence of old familiar sounding brands with just the same products. not good at all. It's like a cheap facsimilie to the current cheap Chinese products in b the market, not worth more than 999\"\n",
        "text = lower(text)\n",
        "text = remove_punctuation(text)\n",
        "text =remove_numbers(text)\n",
        "text =html_tag(text)\n",
        "text =stem_words(text)\n",
        "text =lemmatize_word(text)\n",
        "text =remove_stopwords(text)\n",
        "print(text)\n",
        "text_model.predict([text])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resurg old familiar sound brand product not good like cheap facsimili current cheap chin product b market not worth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['negative'], dtype='<U8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH9DIIy4jGmx",
        "outputId": "7034fcf4-e261-4ed5-88d9-81d72f360398"
      },
      "source": [
        "import joblib\n",
        "joblib.dump(text_model,'SentimentAnalysis_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SentimentAnalysis_model']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdlghdkShr7x",
        "outputId": "8449a1c6-2da1-4751-9d6f-c64bbe448db8"
      },
      "source": [
        "#Accuracy Score\n",
        "text_model.score(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9065333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4sDP521j9S9",
        "outputId": "bab06beb-17a1-47b4-9a17-f9125dedcce3"
      },
      "source": [
        "!pip install streamlit --quiet\n",
        "!pip install pyngrok==4.1.1 --quiet\n",
        "from pyngrok import ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 8.2MB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 51.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2MB 44.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 34.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 55.2MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.4 which is incompatible.\u001b[0m\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvf3zGjlk8VC",
        "outputId": "42087d61-2df9-493a-f74c-5f46410e7d86"
      },
      "source": [
        "%%writefile app.py\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import streamlit as st\n",
        "import sklearn\n",
        "import joblib\n",
        "\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "def html_tag(text):\n",
        "  soup = BeautifulSoup(text,\"html.parser\")\n",
        "  new_text = soup.get_text()\n",
        "  return new_text\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "stop_words.remove('no')\n",
        "stop_words.remove('not')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "def remove_stopwords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    filtered_text = ' '.join(filtered_text)\n",
        "    return filtered_text\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    stems = [stemmer.stem(word) for word in word_tokens]\n",
        "    stems = ' '.join(stems)\n",
        "    return stems\n",
        "  \n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_word(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "  lemmas = ' '.join(lemmas)\n",
        "  return lemmas  \n",
        "\n",
        "def remove_numbers(text):\n",
        "    result = re.sub(r'\\d+', '', text)\n",
        "    return result\n",
        "\n",
        "model = joblib.load('SentimentAnalysis_model')\n",
        "st.title('Sentiment Analyzer')\n",
        "ip = st.text_input(\"Enter The Message\")\n",
        "ip = lower(ip)\n",
        "ip = remove_punctuation(ip)\n",
        "ip = remove_numbers(ip)\n",
        "ip = html_tag(ip)\n",
        "ip = stem_words(ip)\n",
        "ip = lemmatize_word(ip)\n",
        "ip = remove_stopwords(ip)\n",
        "op = model.predict([ip])\n",
        "if st.button(\"Predict\"):\n",
        "  st.write(\"The Given Message Is :- \")\n",
        "  st.title(op[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "82K5tOG5nG6z",
        "outputId": "19ccf60f-1aed-410a-f266-c6258412ed6c"
      },
      "source": [
        "!nohup streamlit run app.py &\n",
        "url = ngrok.connect(port='8501')\n",
        "url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://e0549b29903d.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq6pUfQu4eeY",
        "outputId": "9697a323-96cd-4d6a-869c-5e4cbb7baef3"
      },
      "source": [
        "!pip install pipreqs\n",
        "!pipreqs /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pipreqs\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/83/b1560948400a07ec094a15c2f64587b70e1a5ab5f7b375ba902fcab5b6c3/pipreqs-0.4.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from pipreqs) (0.6.2)\n",
            "Collecting yarg\n",
            "  Downloading https://files.pythonhosted.org/packages/8b/90/89a2ff242ccab6a24fbab18dbbabc67c51a6f0ed01f9a0f41689dc177419/yarg-0.1.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from yarg->pipreqs) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (3.0.4)\n",
            "Installing collected packages: yarg, pipreqs\n",
            "Successfully installed pipreqs-0.4.10 yarg-0.1.9\n",
            "INFO: Successfully saved requirements file in /content/requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}